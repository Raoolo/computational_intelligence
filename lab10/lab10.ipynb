{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for the game\n",
    "BOARD_SIZE = 3  # size of the ttt board\n",
    "# for a board of size 3x3=9 each slot with 3 state we have 3^9 possible states \n",
    "EMPTY = 0       # represents an empty cell on the board\n",
    "PLAYER_X = 1\n",
    "PLAYER_O = -1 \n",
    "REWARDS = {'win': 1.0, 'lose': -1.0, 'draw': 0.5, 'move': 0.0}  # rewards for different outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state):\n",
    "    \"\"\" Function to get the list of available cells of the current state\n",
    "    \n",
    "    Args:\n",
    "        state (np.array): state of the board\n",
    "        \n",
    "    Returns: \n",
    "        list of tuples: (i, j) of available cells\"\"\"\n",
    "    # remember the grid is board_size*board_size\n",
    "    return [(i, j) for i in range(BOARD_SIZE) for j in range(BOARD_SIZE) if state[i, j] == EMPTY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_to_key(state):\n",
    "    \"\"\"Function to convert the board state to a string\n",
    "    Args:\n",
    "        state (np.array): state of the board\n",
    "    \"\"\"\n",
    "    return str(state.reshape(BOARD_SIZE * BOARD_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_action(state, action, player):\n",
    "    \"\"\" Function to apply an action to the board\n",
    "    \n",
    "    Args:\n",
    "        state (np.array): state of the board\n",
    "        action (tuple (i,j)): possible action\n",
    "        player (int): player playing, used to place their mark\n",
    "        \n",
    "    Returns: \n",
    "        new_state (np.array): state of the board\n",
    "    \"\"\"\n",
    "    new_state = state.copy()\n",
    "    new_state[action] = player  \n",
    "    # action  is a tuple so you are accessing in 2 dimension    \n",
    "    # and applying the number of the player associated with that turn to that \n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(state, q_table):\n",
    "    \"\"\" Function to choose the best action based on the Q-values\n",
    "    \n",
    "    Args: \n",
    "        state (np.array): state of the board\n",
    "        q_table (dict): maps states to possible actions and their q-value\n",
    "        \n",
    "    Returns: \n",
    "        best action\"\"\"\n",
    "    # picks all empty cells, which are where the possible actions can be placed (list of tuples (i,j))\n",
    "    actions = available_actions(state)\n",
    "    if not actions: # if no actions available\n",
    "        return None\n",
    "\n",
    "    # get q-values for each possible action\n",
    "    max_q_values = []\n",
    "    for action in actions:\n",
    "        next_state = apply_action(state, action, PLAYER_X)  # apply action to get the next state\n",
    "        next_state_key = board_to_key(next_state)   # convert next state to string key\n",
    "\n",
    "        # get all q-values for the next state and find the maximum q-value\n",
    "        next_state_q_values = q_table.get(next_state_key, {})   # returns empty dict if not found\n",
    "        if next_state_q_values:\n",
    "            max_q_value = max(next_state_q_values.values())\n",
    "        else:   # default to 0 if there are no q-values for the next state\n",
    "            max_q_value = 0  \n",
    "\n",
    "        max_q_values.append(max_q_value)\n",
    "\n",
    "    # return the action corresponding to the highest future reward\n",
    "    return actions[np.argmax(max_q_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_table, epsilon):\n",
    "    \"\"\" Function to choose an action using the epsilon-greedy strategy, meaning it chooses a random action if it is less than epsilon or the best action otherwise\n",
    "\n",
    "    Args:\n",
    "        state (np.array): state of the board\n",
    "        q_table (dict): maps states to possible actions and their q-value\n",
    "        epsilon (float): to balance between exploration/exploitation\n",
    "\n",
    "    Returns:\n",
    "        tuple: The best action or a random action\n",
    "    \"\"\"    \n",
    "    # exploration: choose a random action\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(available_actions(state))\n",
    "    # exploitation: choose the best known action\n",
    "    else:\n",
    "        return best_action(state, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_winner(state):\n",
    "    \"\"\"Function to check if the game is over and return the winner\"\"\"\n",
    "    # check rows, columns, and diagonals for a win\n",
    "    for player in [PLAYER_X, PLAYER_O]:\n",
    "        # first cond: if any row is filled with the player's mark\n",
    "        # second cond: if any column is filled with the player's mark\n",
    "        # third/fourth cond: if any row is filled with the player's mark\n",
    "        if any(np.all(state[i, :] == player) for i in range(BOARD_SIZE)) or \\\n",
    "           any(np.all(state[:, j] == player) for j in range(BOARD_SIZE)) or \\\n",
    "           np.all(np.diag(state) == player) or \\\n",
    "           np.all(np.diag(np.fliplr(state)) == player):\n",
    "            return player   # if you get here it means the player won\n",
    "    # check for a draw \n",
    "    if not available_actions(state):    # no empty spaces left\n",
    "        return 'draw'\n",
    "    # if no win and still available actions remain\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table, state, action, reward, next_state, alpha, gamma):\n",
    "    '''Updates the q_table based on the action done. We want to maximize the discounted (gamma) return of rewards\n",
    "    Args: \n",
    "        q_table (dict): maps states to possible actions and their q-value\n",
    "        state (np.array): state of the board\n",
    "        action (tuple (i,j)): action to be executed\n",
    "        reward (float): reward of the action\n",
    "        next_state (np.array): next state of the board\n",
    "        alpha (float): learning rate of the q_value formula\n",
    "        gamma (float): discount rate\n",
    "    Returns:\n",
    "        q_table (dict): updated q_table duh\n",
    "    '''\n",
    " \n",
    "    key = board_to_key(state)   # get the str which works as a key for q_table\n",
    "    next_key = board_to_key(next_state) # same\n",
    "    \n",
    "    # calculate the maximum q-value for the next state\n",
    "    # if the get does not find the next state's key, it returns an empty dictionary\n",
    "    # which means getting value 0, then you retrieve the maximum value inside the nested dictionary's values\n",
    "    # this wil be used to dictate the next best action\n",
    "    next_max = max(q_table.get(next_key, {}).values(), default=0)   \n",
    "    \n",
    "    # update the Q-value for the current state and action\n",
    "    # formula taken from slide 50 of pack 10 RL, which states:\n",
    "    # watch the agent make action a, transition to a new state s', and receive reward r,\n",
    "    # then update Q-table based on how much (alpha) you want the new action to impact\n",
    "    # an alpha close to 1 will overload the old learning, while close to 0 will give \n",
    "    # more importance to ld experiences and slower convergence, we set it low to give\n",
    "    # a slow and stable learning pattern\n",
    "    q_table.setdefault(key, {})[action] = q_table.get(key, {}).get(action, 0) + \\\n",
    "                                          alpha * (reward + gamma * next_max - q_table.get(key, {}).get(action, 0))\n",
    "    # what the previous line does:\n",
    "    # setdefault returns the value of 'key' or insert the key and then returns {} if it does not exist\n",
    "    # then inside the dictionary access the key 'action' (remember it is a nested dictionary)\n",
    "    # now we update the q_value with the old q_value and the new information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(state):\n",
    "    \"\"\"Function to print the current board state\n",
    "    Args:\n",
    "        state (np.array): state of the board\n",
    "    \"\"\"    \n",
    "    chars = {PLAYER_X: 'X', PLAYER_O: 'O', EMPTY: ' '}\n",
    "    for row in state:\n",
    "        # convert each cell to the str associated and separate them with |\n",
    "        print(' | '.join(chars[cell] for cell in row))\n",
    "        # this is used just to create a horizontal separation line\n",
    "        print('-' * (BOARD_SIZE * 4 - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(games, alpha, gamma, epsilon, epsilon_decay):\n",
    "    \"\"\"Train the player\n",
    "\n",
    "    Args:\n",
    "        games (int): how many games to play\n",
    "        alpha (float): learning rate of the q_value formula\n",
    "        gamma (float): discount rate\n",
    "        epsilon (float): to balance between exploration/exploitation\n",
    "        epsilon_decay (float): how much to decay epsilon\n",
    "\n",
    "    Returns:\n",
    "        q_table (dict): maps states to possible actions and their q-value\n",
    "    \"\"\"    \n",
    "    q_table = {}    # dictionary that stores the state (as a str) and the action's q-values inside nested dictionaries\n",
    "    for game in range(games):\n",
    "        state = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)   # create an empty board\n",
    "        # print(f\"Game {game + 1}:\")\n",
    "        starting_player = random.choice([PLAYER_X, PLAYER_O])\n",
    "        done = False\n",
    "        while not done:\n",
    "            if starting_player == PLAYER_X:\n",
    "                # choose and apply action\n",
    "                action = choose_action(state, q_table, epsilon) # choose either random or best action\n",
    "                state = apply_action(state, action, PLAYER_X)   # apply action to the board, return new board\n",
    "                #print(\"Your move:\")\n",
    "                #print_board(state)\n",
    "                winner = check_winner(state)\n",
    "                if winner or not available_actions(state):\n",
    "                    done = True\n",
    "                else:\n",
    "                    starting_player = PLAYER_O\n",
    "\n",
    "            else:\n",
    "            # opponent's turn, plays following a random approach\n",
    "                action = random.choice(available_actions(state))   # between all actions choose a random one\n",
    "                state = apply_action(state, action, PLAYER_O)  # apply action to the board, return new board\n",
    "                #print(\"Opponent's move:\")\n",
    "                #print_board(state)\n",
    "                winner = check_winner(state)\n",
    "                if winner or not available_actions(state):\n",
    "                    done = True\n",
    "                else:\n",
    "                    starting_player = PLAYER_X\n",
    "\n",
    "            # determine reward and update q-table\n",
    "            # this does not go into an if win since we just check it inside the get, and the move action continues the game without impact\n",
    "            reward = REWARDS.get('win' if winner == PLAYER_X else 'lose' if winner == PLAYER_O else 'draw' if winner == 'draw' else 'move', 0)\n",
    "            update_q_table(q_table, state, action, reward, state, alpha, gamma) # updates q_table based on the action done\n",
    "            if winner is not None or not available_actions(state):  # if there is a winner or there are no more actions available\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "                \n",
    "        # print the outcome of the game\n",
    "        # if winner == PLAYER_X:\n",
    "        #     print(\"Result: player X wins.\")\n",
    "        # elif winner == PLAYER_O:\n",
    "        #     print(\"Result: player O wins.\")\n",
    "        # else:\n",
    "        #     print(\"Result: it's a draw.\")\n",
    "\n",
    "        # diminish epsilon for less exploration over time\n",
    "        epsilon *= epsilon_decay\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "games = 10000\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.9\n",
    "decay = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "q_table = train(games=games, alpha=alpha, gamma=gamma, epsilon=epsilon, epsilon_decay=decay)\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(q_table, num_games=1000):\n",
    "    results = {\"win\": 0, \"loss\": 0, \"draw\": 0}\n",
    "    for _ in range(num_games):\n",
    "        state = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)\n",
    "        while True:\n",
    "            # random player move\n",
    "            random_action = random.choice(available_actions(state))\n",
    "            state = apply_action(state, random_action, PLAYER_O)\n",
    "\n",
    "            winner = check_winner(state)\n",
    "            if winner or not available_actions(state):\n",
    "                results[\"loss\" if winner == PLAYER_X else \"draw\" if winner == 'draw' else \"win\"] += 1\n",
    "                break\n",
    "\n",
    "            # AI move\n",
    "            ai_action = best_action(state, q_table)\n",
    "            state = apply_action(state, ai_action, PLAYER_X)\n",
    "\n",
    "            winner = check_winner(state)\n",
    "            if winner or not available_actions(state):\n",
    "                results[\"win\" if winner == PLAYER_X else \"draw\" if winner == 'draw' else \"loss\"] += 1\n",
    "                break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'win': 971, 'loss': 0, 'draw': 29}\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_agent(q_table=q_table)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
